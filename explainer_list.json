[{
	"key": "96927664-6d53-4b98-be07-eb57359636bc",
	"name": "/Tabular/LIME",
	"explainer_description": "LIME perturbs the input data samples in order to train a simple model that approximates the prediction for the given instance and similar ones. The explanation contains the weight of each attribute to the prediction value. This method accepts 4 arguments: the 'id', the 'instance', the 'url'(optional),  and the 'params' dictionary (optiohnal) with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#LIME",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation contains a plot with the most influent features for the given instance. For regression models, the plot displays both positive and negative contributions of each feature value to the predicted outcome. The same applies to classification models, but there can be a plot for each possible class. A table containing the feature values of the instance is also included.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001480"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#PyTorch", "http://www.w3id.org/iSeeOnto/explainer#Sklearn", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "66520a2c-e922-4976-a979-e908d5d11311",
	"name": "/Images/Anchors",
	"explainer_description": "Uses anchors to find the groups of pixels that are sufficient for the model to justify the predicted class.This method accepts 5 arguments: the 'id', the 'url' (optional),  the 'params' dictionary (optional) with the configuration parameters of the method, the 'instance' containing the image that will be explained as a matrix, or the 'image' file instead. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Anchor",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Anchor_Explanation",
	"explanation_description": "Explanation displays the pixels that are sufficient for the model to justify the outcome",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000081"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001480"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"supportsB&WImage\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "4f3b19c2-c3a7-4a4f-bd0a-4f75aa7b7846",
	"name": "/Images/Counterfactuals",
	"explainer_description": "Finds an image that is similar to the original, but that the model predicts to be from a different class. The class of the conterfactual can be explicitly specified.This method accepts 5 arguments: the 'id', the 'url' (optional),  the 'params' dictionary (optional) with the configuration parameters of the method, the 'instance' containing the image that will be explained as a matrix, or the 'image' file instead. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Wachter",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "https://purl.org/heals/eo#CounterfactualExplanation",
	"explanation_description": "Explanation displays an image that is as similar as possible to the original but that the model predicts to be from a different class.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000081"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"supportsB&WImage\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "321c8fa1-f86d-488c-bbf0-54e8d8284726",
	"name": "/Images/GradCamTorch",
	"explainer_description": "Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept.This method accepts 4 arguments: the 'id', the 'params' dictionary (optional) with the configuration parameters of the method, the 'instance' containing the image that will be explained as a matrix, or the 'image' file instead. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Wachter",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Saliency_Map",
	"explanation_description": "Explanation displays an image that highlights the region that contributes the most to the outcome.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000081"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Linearithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0017046"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#PyTorch"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": false,\\n    \\\"supportsB&WImage\\\": false,\\n    \\\"requiresAttributes\\\": [\\n        {\\n            \\\"target_layer\\\": \\\"name of target layer to be provided as a string. This is the layer that you want to compute the visualization for. Usually this will be the last convolutional layer in the model. It is also possible to specify internal components of this layer by         passing the target_layer_index parameter in params when making a call to the explainer resource. To get the target layer, this method executes 'model.<target_layer>        [<target_layer_index>]' \\\\nSome common examples of these parameters for well-known models:\\\\nResnet18 and 50: model.layer4 -> 'target_layer':'layer4'\\\\nVGG, densenet161: model.features[-1] -> 'target_layer':'features', 'target_layer_index':-1\\\\nmnasnet1_0: model.layers[-1] -> 'target_layer':'layers', 'target_layer_index':-1\\\"\\n        }\\n    ]\\n}\""
}, {
	"key": "e0c39ec9-a64d-4a90-9495-ab1d82d97d96",
	"name": "/Images/LIME",
	"explainer_description": "Uses LIME to identify the group of pixels that contribute the most to the predicted class.This method accepts 5 arguments: the 'id', the 'url' (optional),  the 'params' dictionary (optional) with the configuration parameters of the method, the 'instance' containing the image that will be explained as a matrix, or the 'image' file instead. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#LIME",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation displays the group of pixels that contribute positively (in green) and negatively (in red) to the prediction of the image class. More than one class may be displayed.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000081"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#PyTorch", "http://www.w3id.org/iSeeOnto/explainer#Sklearn", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"supportsB&WImage\\\": false,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "cdb6fac6-5846-4627-88aa-72ea6e1edf70",
	"name": "/Tabular/ALE",
	"explainer_description": "Computes the accumulated local effects (ALE) of a model for the specified features. This method accepts 3 arguments: the 'id', the 'url',  and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#ALE",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presenrs a plot for each of the specified features where the y-axis represents the global feature effect on the outcome value according to the computed ALE values.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#model",
	"presentations": ["http://semanticscience.org/resource/SIO_000904"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Linearithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "0eb99221-7a5e-4a7b-b8e7-ad4799e988ab",
	"name": "/Tabular/Anchors",
	"explainer_description": "Anchors provide local explanations in the form of simple boolean rules with a precision score and a coverage value which represents the scope in which that rules applies to similar instances. This method accepts 4 arguments: the 'id', the 'instance', the 'url' (optional),  and the 'params' JSON (optional) with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Anchor",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Anchor_Explanation",
	"explanation_description": "Explanation displays an instance with the boolean rule (anchor) that was found, and values for its precision and coverage (scope in which that rules applies to similar instances).\t",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000651"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483", "http://purl.obolibrary.org/obo/OMIT_0001480"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "4d1fc9b1-515e-438b-982f-90d6e975c7c9",
	"name": "/Tabular/DicePrivate",
	"explainer_description": "Diverse Counterfactual Explanations (DiCE)  private method generates counterfactuals without training data. However, it requires the format and ranges of the data to be specified when uploading the model. This method is currently supported for TensorFlow models only.  Accepts 3 arguments: the 'id' string, the 'instance', and the 'params' dictionary (optional) containing the configuration parameters of the explainer. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#DiCE",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "https://purl.org/heals/eo#CounterfactualExplanation",
	"explanation_description": "Explanation contains a table with the original instance compared against a generated couterfactual(s).",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": [\\n        {\\n            \\\"features\\\": \\\"Dictionary with feature names as keys and arrays containing the ranges of continuous features, or strings with the categories for categorical features.\\\"\\n        }\\n    ]\\n}\""
}, {
	"key": "94f5dd53-192a-453d-974d-4c972987a724",
	"name": "/Tabular/DicePublic",
	"explainer_description": "Diverse Counterfactual Explanations (DiCE) public method generates counterfactuals using the ML model's training data as a baseline. Accepts 3 arguments: the 'id' string, the 'instance', and the 'params' dictionary (optional) containing the configuration parameters of the explainer. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#DiCE",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "https://purl.org/heals/eo#CounterfactualExplanation",
	"explanation_description": "Explanation contains a table with the original instance compared against a generated couterfactual(s).",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#PyTorch", "http://www.w3id.org/iSeeOnto/explainer#Sklearn", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "6433fd3d-f24f-41d2-859f-07e9f145eb76",
	"name": "/Tabular/DisCERN",
	"explainer_description": "Discovering Counterfactual Explanations using Relevance Features from Neighbourhoods (DisCERN) generates counterfactuals for scikit-learn-based models. Requires 3 arguments: the 'id' string, the 'instance' to be explained, and the 'params' object containing the configuration parameters of the explainer. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#DisCERN",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "https://purl.org/heals/eo#CounterfactualExplanation",
	"explanation_description": "Explanation contains a table with the original instance compared against a generated couterfactual(s).",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Sklearn"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "4a6116a5-cfb2-4c4f-ba6b-6489c4f0c891",
	"name": "/Tabular/Importance",
	"explainer_description": "This method measures the increase in the prediction error of the model after the feature's values are randomly permuted. A feature is considered important if the error of the model increases significantly when permuting it. Accepts 2 arguments: the 'id' string, and the 'params' object (optional) containing the configuration parameters of the explainer. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Feature_Relevance",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation contains a bar plot representing the increase in the prediction error (importance) for the features with the highest values.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "2c3228ec-7764-4a00-9c5a-61f1f410c1be",
	"name": "/Tabular/DeepSHAPGlobal",
	"explainer_description": "This method based on Shapley values computes the average contribution of each feature for the whole training dataset. DeepSHAP is intended for TensorFlow/Keras models only. This method accepts 2 arguments: the 'id', and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presents a beeswarm plot that is designed to display an information-dense summary of how the top features in a dataset impact the model output. Each instance the given explanation is represented by a single dot on each feature fow. The x position of the dot is determined by the SHAP value of that feature, and dots 'pile up' along each feature row to show density. Color is used to display the original value of a feature.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000449"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0017046"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "7aed7f3d-1526-47df-ab2c-92a88e81b37b",
	"name": "/Tabular/DeepSHAPLocal",
	"explainer_description": "This method displays the contribution of each attribute for an individual prediction based on Shapley values (for tree ensemble methods only). Supported for XGBoost, LightGBM, CatBoost, scikit-learn and pyspark tree models. This method accepts 3 arguments: the 'id', the 'instance', and the 'params' dictionary (optional) with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presents a waterfall plot. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000449"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0017046"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "b94da0f8-0583-4b77-83b7-a6ca4d98d152",
	"name": "/Tabular/KernelSHAPLocal",
	"explainer_description": "This method displays the contribution of each attribute for an individual prediction based on Shapley values. This method accepts 4 arguments: the 'id', the 'instance', the 'url' (optional),  and the 'params' dictionary (optional) with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presents a waterfall plot. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000449"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Exponential_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "f87400af-f4ea-4d9d-8de0-27a563f935cb",
	"name": "/Tabular/KernelSHAPGlobal",
	"explainer_description": "This method based on Shapley values computes the average contribution of each feature for the whole training dataset. This method accepts 3 arguments: the 'id', the 'url',  and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presents a beeswarm plot that is designed to display an information-dense summary of how the top features in a dataset impact the model output. Each instance the given explanation is represented by a single dot on each feature fow. The x position of the dot is determined by the SHAP value of that feature, and dots 'pile up' along each feature row to show density. Color is used to display the original value of a feature.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#model",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000449"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Exponential_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "1dcef101-4051-42f7-89c4-0374ebb45280",
	"name": "/Tabular/TreeSHAPGlobal",
	"explainer_description": "This method based on Shapley values computes the average contribution of each feature for the whole training dataset. TreeSHAP is intended for ensemble methods only and is currently supported for XGBoost, LightGBM, CatBoost, scikit-learn and pyspark tree models. This method accepts 2 arguments: the 'id', and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presents a beeswarm plot that is designed to display an information-dense summary of how the top features in a dataset impact the model output. Each instance the given explanation is represented by a single dot on each feature fow. The x position of the dot is determined by the SHAP value of that feature, and dots 'pile up' along each feature row to show density. Color is used to display the original value of a feature.\t",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#model",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000449"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://www.w3id.org/iSeeOnto/aimodel#Ensemble_Method"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#LightGBM", "http://www.w3id.org/iSeeOnto/explainer#Sklearn", "http://www.w3id.org/iSeeOnto/explainer#XGBoost"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "0aa57c62-ea8f-4b0c-a4f5-386b8ed7a6c4",
	"name": "/Tabular/TreeSHAPLocal",
	"explainer_description": "This method displays the contribution of each attribute for an individual prediction based on Shapley values (for tree ensemble methods only). Supported for XGBoost, LightGBM, CatBoost, scikit-learn and pyspark tree models. This method accepts 3 arguments: the 'id', the 'instance', and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation presents a waterfall plot. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904", "http://semanticscience.org/resource/SIO_000449"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://www.w3id.org/iSeeOnto/aimodel#Ensemble_Method"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#LightGBM", "http://www.w3id.org/iSeeOnto/explainer#Sklearn", "http://www.w3id.org/iSeeOnto/explainer#XGBoost"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "104990f6-2430-4bd9-96b4-627f34aa1800",
	"name": "/Tabular/NICE",
	"explainer_description": "NICE is an algorithm to generate Counterfactual Explanations for heterogeneous tabular data.NICE exploits information from a nearest instance to speed up the search process and guarantee that an explanation will be found. Accepts 4 arguments: the 'id' string, the 'instance', the 'url' (optional), and the 'params' dictionary (optional) containing the configuration parameters of the explainer. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Wachter",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "https://purl.org/heals/eo#CounterfactualExplanation",
	"explanation_description": "Explanation contains a table with the original instance compared against a generated couterfactual(s).",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Linearithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "d32478eb-bfb9-4791-8054-9440f5c0523a",
	"name": "/Text/LIME",
	"explainer_description": "LIME perturbs the input data samples in order to train a simple model that approximates the prediction for the given instance and similar ones. The explanation contains the weight of each word to the prediction value. This method accepts 4 arguments: the 'id', the 'instance', the 'url',  and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#LIME",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#text",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "Explanation is a plot with the most influent words for the given instance. For regression models, the plot displays both positive and negative contributions of each word to the predicted outcome. The same applies to classification models, but there can be a plot for each possible class. The text instance with highlighted words is also included.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Linearithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483", "http://purl.obolibrary.org/obo/OMIT_0010354"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "bf54d299-9aac-4e7d-8248-db8281cb1fb8",
	"name": "/Images/IntegratedGradients",
	"explainer_description": "Defines an attribution value for each pixel in the image provided based on the Integration Gradients method. It only works with Tensorflow/Keras models.This method accepts 4 arguments: the 'id', the 'params' dictionary (optional) with the configuration parameters of the method, the 'instance' containing the image that will be explained as a matrix, or the 'image' file that can be passed instead of the instance. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Integrated_Gradient_Technique",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Contrasting_Feature_Importance_Explanation",
	"explanation_description": "Subplot with four columns. The first column shows the original image and its prediction. The second column shows the values of the attributions for the target class. The third column shows the positive valued attributions. The fourth column shows the negative valued attributions.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelClassSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000907"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0017046"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"supportsB&WImage\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "a1a4d020-173c-42e9-b03d-7bad3645b0ee",
	"name": "/Tabular/IREX",
	"explainer_description": "IREX is a reusable method for the Iterative Refinement and EXplanation of classification models. It has been designed for domain-expert users -without machine learning skills- that need to understand and improve classification models. This particular implementation of IREX uses ALE to identify anomalous features that may be contradictory to what the expert knowledge indicates. Anomalous features are highlighted in red in an ALE heatmap. This method accepts 3 arguments: the 'id', the 'url',  and the 'params' JSON with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#ALE",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "A heatmap displaying the relevance of the features according to ALE, where anomalous features (behavior differring from expected values) are highlighted in red.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#model",
	"presentations": ["http://semanticscience.org/resource/SIO_000904"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Linearithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001480"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "c88b3d73-0ed4-4fa3-9e18-0b2e0c5cfd20",
	"name": "/Text/NLPClassifier",
	"explainer_description": "An explainer for NLP classification models. ",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Data-driven",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#text",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Feature_Influence_Explanation",
	"explanation_description": "The explanation shows the confidence scores for the possible classes of the text classified. The explanation also shows the top keywords used in the query with the TF-IDF score, the top keywords used in similar texts per class, and the overlapping words with similar texts for each class.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000651", "http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0010354", "http://purl.obolibrary.org/obo/OMIT_0001480"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Multi-class_Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Sklearn"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "a4b832e9-bc74-4670-b318-e8c10e5bf2f7",
	"name": "/Timeseries/CBRFox",
	"explainer_description": "This method applies the Case-Based Reasoning paradigm to provide explanations-by-example, where time series are split into different time-window cases that serveas explanation cases for the outcome of the prediction model. It has been designed for domain-expert users -without ML skills- that need to understand and how (future)predictions could be dependent of past time series windows. It proposes a novel similarity function which deals with both the morphological similarity and the absoluteproximity between the time series, together with several reuse strategies to generate the explanation cases. It uses an automatic evaluation approach based on computingthe error (MAE) between the model prediction for and the actual values in the solution of the explanatory case. Finally, this evaluation method is applied to demonstratethe performance of the proposal on the given dataset. This method accepts 3 arguments: the 'id', the 'instance',  and the 'params' dictionary (optional) with the configuration parameters of the method. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Knowledge_Extraction",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#time_series",
	"explanation_type": "https://purl.org/heals/eo#CaseBasedExplanation",
	"explanation_description": "The explanation shown is an explanation-by-example where users can watch charts comparing meteorogical features between the case to explain and its most similar case, or between the case and its least similar case. ",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000904"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Quadratic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0010354", "http://purl.obolibrary.org/obo/OMIT_0001480"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Forecasting"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": true,\\n    \\\"requiresAttributes\\\": [\\n        {\\n            \\\"target_columns\\\": \\\"Array containing the indexes of the target columns of the dataset.\\\"\\n        }\\n    ]\\n}\""
}, {
	"key": "6b7b9c08-8bbf-43a7-b13d-0394415160ae",
	"name": "/Images/GradCam",
	"explainer_description": "Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept.This method accepts 4 arguments: the 'id', the 'params' dictionary (optional) with the configuration parameters of the method, the 'instance' containing the image that will be explained as a matrix, or the 'image' file instead. These arguments are described below.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#GradCam_Technique",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Saliency_Map",
	"explanation_description": "Explanation displays an image that highlights the region that contributes the most to the outcome.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000081"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Linearithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0017046"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#PyTorch", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": false,\\n    \\\"needsData\\\": false,\\n    \\\"supportsB&WImage\\\": false,\\n    \\\"requiresAttributes\\\": [\\n        {\\n            \\\"target_layer\\\": \\\"name of target layer to be provided as a string. This is the layer that you want to compute the visualization for. Usually this will be the last convolutional layer in the model. It is also possible to specify internal components of this layer by         passing the target_layer_index parameter in params when making a call to the explainer resource. To get the target layer, this method executes 'model.<target_layer>        [<target_layer_index>]' \\\\nSome common examples of these parameters for well-known models:\\\\nResnet18 and 50: model.layer4 -> 'target_layer':'layer4'\\\\nVGG, densenet161: model.features[-1] -> 'target_layer':'features', 'target_layer_index':-1\\\\nmnasnet1_0: model.layers[-1] -> 'target_layer':'layers', 'target_layer_index':-1\\\"\\n        }\\n    ]\\n}\""
}, {
	"key": "ffc2eaca-7f98-43fa-8128-aed6394d6031",
	"name": "/Images/NearestNeighbours",
	"explainer_description": "Finds the nearest neighbours to a data instances based on minimum euclidean distance",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Data-driven",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "http://www.w3id.org/iSeeOnto/explainer#Neighbourhood_Explanation",
	"explanation_description": "This explanation presents nearest neighbours to the query; nearest neighbours are examples that are similar to the query with similar AI system outcomes.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelClassSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#local",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["http://semanticscience.org/resource/SIO_000081"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Exponential_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0017046"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Classification", "http://www.w3id.org/iSeeOnto/aimodel#Regression"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#PyTorch", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow1", "http://www.w3id.org/iSeeOnto/explainer#TensorFlow2"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"supportsB&WImage\\\": false,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "036446b7-6c4f-41af-887a-725a694b92c4",
	"name": "/Misc/AIModePerformance",
	"explainer_description": "Rule based explainer that extracts the performance metrics from case structure representation of the use case.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Knowledge_Extraction",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#image",
	"explanation_type": "https://purl.org/heals/eo#StatisticalExplanation",
	"explanation_description": "This explanation presents the perfromance metrics of the AI System.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#model",
	"presentations": ["http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Constant_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["https://purl.org/heals/eo#InductiveTask"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "0b69ee85-9c21-41eb-a27c-c840dfaf4a7c",
	"name": "/Misc/AIModelPerformance",
	"explainer_description": "Rule based explainer that extracts the performance metrics from case structure representation of the use case.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#Data-driven",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "https://purl.org/heals/eo#StatisticalExplanation",
	"explanation_description": "This explanation presents the perfromance metrics of the AI System.",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#model-agnostic",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#global",
	"target": "http://www.w3id.org/iSeeOnto/explainer#model",
	"presentations": ["http://semanticscience.org/resource/SIO_000419"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Constant_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["https://purl.org/heals/eo#InductiveTask"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"supportsAPI\\\": true,\\n    \\\"needsData\\\": false,\\n    \\\"requiresAttributes\\\": []\\n}\""
}, {
	"key": "47a377d9-e4aa-46ad-bdab-0fe6a6c58f2a",
	"name": "/Tabular/PertCF",
	"explainer_description": "PertCF is a perturbation-based counterfactual generation method that benefits from the feature attributions generated by the SHAP.                               PertCF combines the strengths of perturbation-based counterfactual                               generation and feature attribution to generate high-quality, stable,                               and interpretable counterfactuals. This method accepts 3 arguments:                               'id', 'instance', and the execution 'params'.",
	"technique": "http://www.w3id.org/iSeeOnto/explainer#SHAP",
	"dataset_type": "http://www.w3id.org/iSeeOnto/explainer#multivariate",
	"explanation_type": "https://purl.org/heals/eo#CounterfactualExplanation",
	"explanation_description": "PertCF",
	"concurrentness": "http://www.w3id.org/iSeeOnto/explainer#post-hoc",
	"portability": "http://www.w3id.org/iSeeOnto/explainer#modelClassSpecific",
	"scope": "http://www.w3id.org/iSeeOnto/explainer#cohort",
	"target": "http://www.w3id.org/iSeeOnto/explainer#prediction",
	"presentations": ["https://purl.org/heals/eo#CounterfactualExplanation"],
	"computational_complexity": "http://www.w3id.org/iSeeOnto/explainer#Logarithmic_time",
	"ai_methods": ["http://purl.obolibrary.org/obo/OMIT_0001483"],
	"ai_tasks": ["http://www.w3id.org/iSeeOnto/aimodel#Binary_Classification", "http://www.w3id.org/iSeeOnto/aimodel#Multi-class_Classification"],
	"implementation": ["http://www.w3id.org/iSeeOnto/explainer#Any"],
	"metadata": "\"{\\n    \\\"modelAccess\\\": \\\"File\\\",\\n    \\\"supportsBWImage\\\": false,\\n    \\\"needsTrainingData\\\": true\\n}\""
}]